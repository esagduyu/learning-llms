# Learning how to build my own large(-ish) language models from scratch

My attempt at recreating some ML learnings as I try to learn how to build LLMs from scratch.

Initially I will be going through Karpathy's YouTube lecture series called [Neural Networks: Zero to Hero](https://github.com/karpathy/nn-zero-to-hero/tree/master). We will see how far I will get...

*(also using this as an excuse to set up [WSL](https://learn.microsoft.com/en-us/windows/wsl/) on my Desktop PC and learning how to use GPU acceleration on it)*

## Useful resources I found along the way

This is mainly here to force me to stop hoarding them in open tabs :)

* Karpathy's course & repo (linked above)
* WSL setup (linked above)
* [Maximme Labonne's LLM Course](https://github.com/mlabonne/llm-course) for a meta-list of learning paths
* Fast.ai's [Practical Deep Learning for Coders](https://course.fast.ai/), especially [its new Part II](https://course.fast.ai/Lessons/part2.html)
* HuggingFace's [Transformers Docs](https://huggingface.co/docs/transformers/index)
